{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d0bf03c-3b3d-479f-9f59-50ad4feb8279",
   "metadata": {},
   "source": [
    "# import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f448e7c7-ff13-4fd5-9ea1-066fdf364121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling, transform_bounds\n",
    "from rasterio.io import MemoryFile\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors as mcolors\n",
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "from scipy.stats import entropy, mode\n",
    "from skimage.util.shape import view_as_blocks\n",
    "import pyproj\n",
    "import utm\n",
    "from affine import Affine\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import cv2\n",
    "from joblib import Parallel, delayed\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a45bf-5c57-4395-986c-0e7fe9c36497",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8438f9-02a6-4d92-b406-986269d18156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_topojson(topojson_path, src_crs=4326):\n",
    "    # Load the TopoJSON file directly with GeoPandas\n",
    "    gdf = gpd.read_file(topojson_path)\n",
    "    \n",
    "    # Manually set the CRS to EPSG:4326\n",
    "    gdf.set_crs(epsg=src_crs, inplace=True)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def reproject_to_utm_and_calculate_area(gdf):\n",
    "    ''' Use the centroid to calculate the UTM zone \n",
    "    '''\n",
    "    # Temporary reprojection to an equal area projection to calculate the centroid\n",
    "    gdf_equal_area = gdf.to_crs(epsg=6933) # EPSG:6933 is the World Equidistant Cylindrical projection\n",
    "    centroid_equal_area = gdf_equal_area.geometry.union_all().centroid\n",
    "    gdf_temp = gpd.GeoDataFrame(geometry=gpd.GeoSeries([centroid_equal_area]))\n",
    "    gdf_temp.set_crs(epsg=6933, inplace=True)    \n",
    "    gdf_temp = gdf_temp.to_crs(epsg=4326) # convert back to geographic coordinates \n",
    "    lon,lat = gdf_temp.geometry[0].coords[0]\n",
    "    \n",
    "    x = utm.from_latlon(lat, lon) # easting, northing, zone_number, band_letter\n",
    "    \n",
    "    def infer_utm_epsg(utm_tuple):\n",
    "        easting, northing, zone_number, band_letter = utm_tuple\n",
    "        # Determine the hemisphere from the band letter\n",
    "        if 'N' <= band_letter <= 'X':  # Northern Hemisphere. Code starts with 326\n",
    "            epsg_code = 32600 + zone_number \n",
    "        elif 'C' <= band_letter <= 'M':  # Southern Hemisphere. Code starts with 327\n",
    "            epsg_code = 32700 + zone_number\n",
    "        else:\n",
    "            raise ValueError(\"Invalid band letter\")\n",
    "        return epsg_code\n",
    "\n",
    "    epsg_code = infer_utm_epsg(x)\n",
    "    \n",
    "    # Reproject the original GeoDataFrame to the determined UTM zone\n",
    "    gdf_utm = gdf.to_crs(epsg=epsg_code)\n",
    "\n",
    "    # Calculate the area of the reprojected GeoDataFrame\n",
    "    gdf_utm['area'] = gdf_utm.geometry.area\n",
    "\n",
    "    # Calculate the total area\n",
    "    total_area = gdf_utm['area'].sum()\n",
    "\n",
    "    return epsg_code, total_area, (lat, lon)\n",
    "\n",
    "def align_rasters(dem_path, land_cover_path, dst_crs, resolution=None):\n",
    "    # calculate destination affine, width and height of dem and land cover in the destination coordiantes\n",
    "    with rasterio.open(dem_path) as dem_src:\n",
    "        dem_transform, dem_width, dem_height = calculate_default_transform(\n",
    "            dem_src.crs, dst_crs, dem_src.width, dem_src.height, *dem_src.bounds)\n",
    "        dem_bounds = transform_bounds(dem_src.crs, dst_crs, *dem_src.bounds)\n",
    "        \n",
    "    with rasterio.open(land_cover_path) as lc_src:\n",
    "        lc_transform, lc_width, lc_height = calculate_default_transform(\n",
    "            lc_src.crs, dst_crs, lc_src.width, lc_src.height, *lc_src.bounds)\n",
    "        lc_bounds = transform_bounds(lc_src.crs, dst_crs, *lc_src.bounds)\n",
    "\n",
    "    # Calculate intersection bounds\n",
    "    intersection_bounds = (\n",
    "        max(dem_bounds[0], lc_bounds[0]),\n",
    "        max(dem_bounds[1], lc_bounds[1]),\n",
    "        min(dem_bounds[2], lc_bounds[2]),\n",
    "        min(dem_bounds[3], lc_bounds[3])\n",
    "    )\n",
    "\n",
    "    # Calculate intersection transform and dimensions\n",
    "    # both lc_width/height and dem_width/height can be used because it only takes the intersection.\n",
    "    intersection_transform, intersection_width, intersection_height = calculate_default_transform(\n",
    "        dst_crs, dst_crs, dem_width, dem_height, *intersection_bounds, resolution=resolution) \n",
    "    \n",
    "    # reprojection\n",
    "    with rasterio.open(dem_path) as dem_src:\n",
    "        dem_reprojected = reproject_and_resample(\n",
    "            dem_src, dst_crs, intersection_transform, intersection_width, intersection_height)\n",
    "\n",
    "    with rasterio.open(land_cover_path) as lc_src:\n",
    "        lc_reprojected = reproject_and_resample(\n",
    "            lc_src, dst_crs, intersection_transform, intersection_width, intersection_height)\n",
    "        \n",
    "    return dem_reprojected, lc_reprojected, intersection_transform\n",
    "\n",
    "def reproject_and_resample(src, dst_crs, dst_transform, dst_width, dst_height, resampling=Resampling.nearest):\n",
    "    dst_shape = (dst_height, dst_width)\n",
    "    dst_array = np.empty(dst_shape, dtype=src.read(1).dtype)\n",
    "    reproject(\n",
    "        source=src.read(1),\n",
    "        destination=dst_array,\n",
    "        src_transform=src.transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_transform=dst_transform,\n",
    "        dst_crs=dst_crs,\n",
    "        resampling=resampling\n",
    "    )\n",
    "    return dst_array\n",
    "\n",
    "def reclassify_land_cover_urban_forest(land_cover):    \n",
    "    # Define the reclassification dictionary from Level II to Level I\n",
    "    reclass_dict = {\n",
    "        11: 0, 12: 0, 31: 0, 51: 0, 52: 0, 71: 0, 72: 0, 73: 0, 74: 0, 81: 0, 82: 0, 90: 0, 95: 0, # Others\n",
    "        21: 2, 22: 2, 23: 2, 24: 2,  # Developed\n",
    "        41: 4, 42: 4, 43: 4,  # Forest\n",
    "    }\n",
    "    reclass_land_cover = np.copy(land_cover)\n",
    "    for key, value in reclass_dict.items():\n",
    "        reclass_land_cover[land_cover == key] = value\n",
    "    return reclass_land_cover\n",
    "\n",
    "# Calculate slope in degrees \n",
    "def calculate_slope(dem, transform):\n",
    "    x, y = np.gradient(dem, transform[0], transform[4]) # reflect real world distance\n",
    "    slope = np.arctan(np.sqrt(x**2 + y**2)) * 180 / np.pi\n",
    "    return slope\n",
    "\n",
    "# Classify terrain based on slope (USDA Slope Gradient Classification)\n",
    "def classify_slope(slope):\n",
    "    classification = np.zeros_like(slope)\n",
    "    classification[(slope >= 0) & (slope < 2)] = 1  # Flat: <2 degrees\n",
    "    classification[(slope >= 2) & (slope < 5)] = 2  # Undulating: 2-5 degrees\n",
    "    classification[(slope >= 5) & (slope < 8)] = 3  # Moderately sloping: 5-8 degrees\n",
    "    classification[(slope >= 8) & (slope < 17)] = 4  # Hilly: 8-17 degrees\n",
    "    classification[(slope >= 17) & (slope < 24)] = 5  # Moderately steep: 17-24 degrees\n",
    "    classification[(slope >= 24) & (slope < 33)] = 6  # Steep: 24-33 degrees\n",
    "    classification[slope >= 33] = 7  # Very steep: >33 degrees\n",
    "    return classification\n",
    "\n",
    "def reclassify_slope(slope):\n",
    "    reclass_dict = {\n",
    "        2:1,\n",
    "        3:2,4:2,\n",
    "        5:3,6:3,7:3\n",
    "    }\n",
    "    reclass_slope = np.copy(slope)\n",
    "    for key, value in reclass_dict.items():\n",
    "        reclass_slope[slope == key] = value\n",
    "    return reclass_slope\n",
    "\n",
    "def process_maps_class_freq(slope_map, land_cover_map, tile_size, ignore_non_aoi=0.8):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    slope_map (2D np.array): Slope classification map\n",
    "    land_cover_map (2D np.array): Land cover map\n",
    "    tile_size (int): Size of each tile in pixels (assuming square tiles)\n",
    "    \n",
    "    Returns:\n",
    "    entropy_map (2D np.array): Map of joint entropy values\n",
    "    \"\"\"\n",
    "    # Ensure maps are the same size\n",
    "    assert slope_map.shape == land_cover_map.shape, \"Maps must be the same size\"\n",
    "    \n",
    "    # Adjust dimensions to be multiples of tile_size\n",
    "    rows, cols = slope_map.shape\n",
    "    rows_trimmed = (rows // tile_size) * tile_size\n",
    "    cols_trimmed = (cols // tile_size) * tile_size\n",
    "    slope_map = slope_map[:rows_trimmed, :cols_trimmed]\n",
    "    land_cover_map = land_cover_map[:rows_trimmed, :cols_trimmed]\n",
    "    \n",
    "    # Split the maps into tiles\n",
    "    slope_tiles = view_as_blocks(slope_map, block_shape=(tile_size, tile_size)) # (h, w, tilesize, tilesize)\n",
    "    land_cover_tiles = view_as_blocks(land_cover_map, block_shape=(tile_size, tile_size))\n",
    "    h,w, _, _ = land_cover_tiles.shape\n",
    "    mask = np.sum(land_cover_tiles>0, axis=(-2,-1)) < (tile_size**2 * ignore_non_aoi) # identify where more than XX% pixels are Non AoI class (id: 0)\n",
    "    \n",
    "    # get most frequent class as coarse classification map for tile images\n",
    "    coarse_lc = land_cover_tiles.reshape(h, w, -1)\n",
    "    coarse_lc = mode(coarse_lc, axis=-1)[0]\n",
    "    coarse_slope = slope_tiles.reshape(h, w, -1)\n",
    "    coarse_slope = mode(coarse_slope, axis=-1)[0]\n",
    "    combined = np.concatenate([coarse_lc[..., np.newaxis], coarse_slope[..., np.newaxis]], -1) # (h,w,2)\n",
    "    combined_flatten = combined.reshape(-1, 2)\n",
    "    u, inv, c = np.unique(combined_flatten, axis=0, return_counts=True, return_inverse=True)\n",
    "    inv_freq = 1 / c\n",
    "    combined_inv_freq_map = inv_freq[inv].reshape(h, w)\n",
    "    \n",
    "    # compute probability map\n",
    "    combined_inv_freq_map[mask] = 0 # ignore > XX% non AoI region\n",
    "    if np.sum(combined_inv_freq_map) == 0:\n",
    "        num_patches_available = 0\n",
    "        return None, slope_tiles, land_cover_tiles, num_patches_available, mask\n",
    "    else:\n",
    "        combined_inv_prob_map = combined_inv_freq_map / np.sum(combined_inv_freq_map) # convert them to probability\n",
    "        num_patches_available = np.sum(combined_inv_prob_map > 0)\n",
    "        return combined_inv_prob_map, slope_tiles, land_cover_tiles, num_patches_available, mask\n",
    "\n",
    "\n",
    "def sample_pixels_by_probability(probability_map, num_samples):\n",
    "    h, w = probability_map.shape\n",
    "    n_total_pixels = h * w\n",
    "    # only performs sampling on non-zero probability tiles \n",
    "    mask = (probability_map > 0).reshape(-1)\n",
    "    n_valid_patches = np.sum(mask)\n",
    "    flat_index = np.arange(n_total_pixels)\n",
    "    if num_samples is None or num_samples >= n_valid_patches: \n",
    "        # sort the samples with probability. All samples are chosen and ranked.\n",
    "        sorted_index = np.random.choice(flat_index, n_valid_patches, replace=False, p=probability_map.reshape(-1))\n",
    "        sampled_indices_2d = np.unravel_index(sorted_index, (h, w)) # take all non-zero prob tiles\n",
    "        # conver to nsample tuple where each element is an array [index_h, index_w]\n",
    "        array_h, array_w = sampled_indices_2d\n",
    "        sampled_indices_2d = [np.array([x, y]) for (x,y) in zip(array_h, array_w) ]\n",
    "        return sampled_indices_2d\n",
    "    elif num_samples < n_valid_patches:  \n",
    "        # sample based on the prob map\n",
    "        sampled_indices = np.random.choice(flat_index, num_samples, replace=False, p=probability_map.reshape(-1))\n",
    "        sampled_indices_2d = np.unravel_index(sampled_indices, (h, w)) # tuple of array ((nsample,), (nsample,))\n",
    "        # conver to nsample tuple where each element is an array [index_h, index_w]\n",
    "        array_h, array_w = sampled_indices_2d\n",
    "        sampled_indices_2d = [np.array([x, y]) for (x,y) in zip(array_h, array_w) ]\n",
    "        return sampled_indices_2d\n",
    "    elif n_valid_patches == 0:\n",
    "        return []\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for num_samples\")\n",
    "\n",
    "def back_trace_tiles(indices, original_tiles, tile_size, transform):\n",
    "    \"\"\"\n",
    "    Back trace the selected pixels to the original tiles and get their geographical bounds.\n",
    "    \n",
    "    Args:\n",
    "    indices (list of tuples): Indices of the selected pixels in the entropy map.\n",
    "    original_tiles (3D np.array): The original map tiles to trace back to.\n",
    "    tile_size (int): The size of the tiles in the original map.\n",
    "    transform (Affine): Affine transform for the map.\n",
    "    \n",
    "    Returns:\n",
    "    traced_tiles (list of 2D np.array): List of tiles from the original map at the specified indices.\n",
    "    bounds (list of tuples): List of geographical bounds for each tile.\n",
    "    \"\"\"\n",
    "    traced_tiles = [original_tiles[tuple(index)] for index in indices] # extract the chosen tiles by sampled index \n",
    "    bounds = [ # get the geographical tile bounds based on sampled index. \n",
    "        (\n",
    "            transform * (index[1] * tile_size, index[0] * tile_size),  # (min_x, min_y)\n",
    "            transform * ((index[1] + 1) * tile_size, (index[0] + 1) * tile_size)  # (max_x, max_y)\n",
    "        )\n",
    "        for index in indices\n",
    "    ]\n",
    "    return traced_tiles, bounds\n",
    "\n",
    "def geo_bounds_to_pixel_indices(bounds, transform):\n",
    "    \"\"\"\n",
    "    Convert geographical bounds to image pixel indices.\n",
    "    \n",
    "    Args:\n",
    "    bounds (tuple): Geographical bounds as (min_x, min_y, max_x, max_y).\n",
    "    transform (Affine): Affine transform for the map.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Pixel indices as (min_row, min_col, max_row, max_col).\n",
    "    \"\"\"\n",
    "    min_x, min_y, max_x, max_y = bounds\n",
    "    \n",
    "    # Use the inverse of the transform to map coordinates to pixels\n",
    "    inv_transform = ~transform\n",
    "    \n",
    "    # Calculate the pixel coordinates for the min and max bounds\n",
    "    min_col, min_row = inv_transform * (min_x, min_y)\n",
    "    max_col, max_row = inv_transform * (max_x, max_y)\n",
    "    \n",
    "    # Convert to integer pixel indices\n",
    "    min_row, min_col = int(np.floor(min_row)), int(np.floor(min_col))\n",
    "    max_row, max_col = int(np.ceil(max_row)), int(np.ceil(max_col))\n",
    "    \n",
    "    return min_row, min_col, max_row, max_col\n",
    "\n",
    "def get_tile_labels(tile_list):\n",
    "    tile_labels = [mode(t.ravel()).mode for t in tile_list]\n",
    "    return tile_labels\n",
    "\n",
    "def transform_bounds_to_epsg3857(bounds_list, source_epsg):\n",
    "    # Define the source and target CRS\n",
    "    source_crs = f\"EPSG:{source_epsg}\"\n",
    "    target_crs = \"EPSG:3857\"\n",
    "    \n",
    "    # Create a transformer object\n",
    "    transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "    \n",
    "    transformed_bounds_list = []\n",
    "    \n",
    "    for bounds in bounds_list:\n",
    "        # Unpack the bounds\n",
    "        (minx, miny), (maxx, maxy) = bounds\n",
    "        \n",
    "        # Transform the coordinates\n",
    "        minx_3857, miny_3857 = transformer.transform(minx, miny)\n",
    "        maxx_3857, maxy_3857 = transformer.transform(maxx, maxy)\n",
    "        \n",
    "        # Append the transformed bounds to the list\n",
    "        transformed_bounds_list.append(((minx_3857, miny_3857), (maxx_3857, maxy_3857)))\n",
    "    \n",
    "    return transformed_bounds_list\n",
    "\n",
    "\n",
    "def to_json_str(lst):\n",
    "    \"\"\"\n",
    "    Converts a nested list containing np.float32 elements to a JSON string.\n",
    "\n",
    "    Parameters:\n",
    "    lst (list): A nested list containing np.float32 values.\n",
    "\n",
    "    Returns:\n",
    "    str: A JSON string representation of the list.\n",
    "    \"\"\"\n",
    "    # Convert np.float32 elements to standard Python float\n",
    "    def convert_element(element):\n",
    "        if isinstance(element, np.float32):\n",
    "            return int(element)\n",
    "        elif isinstance(element, list):  # Recursively convert nested lists\n",
    "            return [convert_element(e) for e in element]\n",
    "        elif isinstance(element, np.uint8):\n",
    "            return int(element)\n",
    "        return element  # Return as-is for other types\n",
    "\n",
    "    converted_list = convert_element(lst)\n",
    "    return json.dumps(converted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c76519ec-033e-41e7-ac61-8e36481eb22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_raster_to_match(src_raster, dst_crs):\n",
    "    \"\"\"\n",
    "    Reproject a raster to match the given destination CRS.\n",
    "\n",
    "    Parameters:\n",
    "    src_raster (rasterio.DatasetReader): The source raster to be reprojected.\n",
    "    dst_crs (rasterio.crs.CRS): The destination CRS.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The reprojected raster data.\n",
    "    rasterio.Affine: The transform of the reprojected raster.\n",
    "    \"\"\"\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src_raster.crs, dst_crs, src_raster.width, src_raster.height, *src_raster.bounds)\n",
    "    kwargs = src_raster.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "\n",
    "    with MemoryFile() as memfile:\n",
    "        with memfile.open(**kwargs) as reprojected_raster:\n",
    "            for i in range(1, src_raster.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src_raster, i),\n",
    "                    destination=rasterio.band(reprojected_raster, i),\n",
    "                    src_transform=src_raster.transform,\n",
    "                    src_crs=src_raster.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)\n",
    "\n",
    "            reprojected_data = reprojected_raster.read()\n",
    "            reprojected_transform = reprojected_raster.transform\n",
    "    return reprojected_data, reprojected_transform\n",
    "\n",
    "def load_and_visualize_rasters(raster_path1, raster_path2):\n",
    "    \"\"\"\n",
    "    Loads and visualizes two raster files side by side.\n",
    "\n",
    "    Parameters:\n",
    "    raster_path1 (str): The path to the first raster file.\n",
    "    raster_path2 (str): The path to the second raster file.\n",
    "    \"\"\"\n",
    "    # Open the first raster file (land cover)\n",
    "    with rasterio.open(raster_path1) as src1:\n",
    "        raster1 = src1.read(1)\n",
    "        transform1 = src1.transform\n",
    "        crs1 = src1.crs\n",
    "\n",
    "    # Open the second raster file (DEM)\n",
    "    with rasterio.open(raster_path2) as src2:\n",
    "        raster2, transform2 = reproject_raster_to_match(src2, crs1)\n",
    "\n",
    "    # Define a colormap for the land cover map\n",
    "    # cmap = ListedColormap([\n",
    "    #     'black',         # outside the polygon\n",
    "    #     'blue',          # Open Water (11)\n",
    "    #     'white',         # Perennial Ice/Snow (12)\n",
    "    #     'lightcoral',    # Developed, open space (21)\n",
    "    #     'indianred',     # Developed, low intensity (22)\n",
    "    #     'brown',         # Developed, medium intensity (23)\n",
    "    #     'darkred',       # Developed, high intensity (24)\n",
    "    #     'peru',          # Barren land (31)\n",
    "    #     'forestgreen',   # Deciduous forest (41)\n",
    "    #     'darkgreen',     # Evergreen forest (42)\n",
    "    #     'limegreen',     # Mixed forest (43)\n",
    "    #     'yellowgreen',   # Shrubland (52)\n",
    "    #     'yellow',        # Grassland (71)\n",
    "    #     'lightyellow',   # Pasture/hay (81)\n",
    "    #     'gold',          # Cropland (82)\n",
    "    #     'saddlebrown',   # Woody wetland (90)\n",
    "    #     'lightseagreen'  # Herbaceous wetland (95)\n",
    "    # ])\n",
    "    cmap = mcolors.ListedColormap([\n",
    "            'black', # not in AoI\n",
    "            'blue', 'white',  # Water\n",
    "            'lightcoral', 'coral', 'tomato', 'orangered',  # Developed\n",
    "            'gray',  # Barren\n",
    "            'darkgreen', 'forestgreen', 'limegreen',  # Forest\n",
    "            'darkkhaki', 'khaki',  # Shrubland\n",
    "            'lightgoldenrodyellow', 'gold', 'goldenrod', 'darkgoldenrod',  # Herbaceous\n",
    "            'lightgreen', 'lime',  # Planted/Cultivated\n",
    "            'lightcyan', 'cyan'  # Wetlands\n",
    "        ])\n",
    "\n",
    "    # Define the legend labels\n",
    "    legend_labels = [\n",
    "        'outside of AoI', 'Open Water (11)', 'Perennial Ice/Snow (12)', 'Developed, open space (21)',\n",
    "        'Developed, low intensity (22)', 'Developed, medium intensity (23)', 'Developed, high intensity (24)',\n",
    "        'Barren land (31)', 'Deciduous forest (41)', 'Evergreen forest (42)',\n",
    "        'Mixed forest (43)', 'Shrubland (52)', 'Grassland (71)',\n",
    "        'Pasture/hay (81)', 'Cropland (82)', 'Woody wetland (90)', 'Herbaceous wetland (95)'\n",
    "    ]\n",
    "\n",
    "    # Create a patch for each color\n",
    "    legend_patches = [mpatches.Patch(color=cmap(i), label=label) for i, label in enumerate(legend_labels)]\n",
    "\n",
    "    # Plot the rasters side by side\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    fig, axes = plt.subplots(2,1, figsize=(15, 10))\n",
    "\n",
    "    # Plot the first raster (land cover)\n",
    "    ax1 = axes[0]\n",
    "    img1 = ax1.imshow(raster1, cmap=cmap, extent=[\n",
    "        transform1[2], \n",
    "        transform1[2] + transform1[0] * raster1.shape[1], \n",
    "        transform1[5] + transform1[4] * raster1.shape[0], \n",
    "        transform1[5]\n",
    "    ])\n",
    "    ax1.set_title('Land Cover Map')\n",
    "    ax1.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    print(np.unique(raster1))\n",
    "    # Plot the second raster (DEM)\n",
    "    ax2 = axes[1]\n",
    "    img2 = ax2.imshow(raster2[0], cmap='terrain', extent=[\n",
    "        transform2[2], \n",
    "        transform2[2] + transform2[0] * raster2.shape[2], \n",
    "        transform2[5] + transform2[4] * raster2.shape[1], \n",
    "        transform2[5]\n",
    "    ])\n",
    "    ax2.set_title('Digital Elevation Model')\n",
    "    plt.colorbar(img2, ax=ax2, orientation='vertical')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746270d4-a4a5-4292-a025-4e7359d4cfea",
   "metadata": {},
   "source": [
    "# sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff646a04-39ff-4c29-a817-183adf528224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 2054 entries, 37 to 2117\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   id        2054 non-null   object  \n",
      " 1   count     2054 non-null   int64   \n",
      " 2   name      2054 non-null   object  \n",
      " 3   url       2054 non-null   object  \n",
      " 4   geometry  2054 non-null   geometry\n",
      "dtypes: geometry(1), int64(1), object(3)\n",
      "memory usage: 96.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "topojson_path = '../../data/point_cloud_boundary/20240627.topojson'\n",
    "nlcd_all = glob('../../data/NLCD_clip/*.tif')\n",
    "dem_all = glob('../../data/3DEP_30m_clip/*.tif')\n",
    "name_list_nlcd_all = ['_'.join(x.split('/')[-1].split('_')[:-2]) for x in nlcd_all]\n",
    "name_list_dem_all = ['_'.join(x.split('/')[-1].split('_')[:-1]) for x in dem_all]\n",
    "\n",
    "# filter \n",
    "gdf = load_topojson(topojson_path)\n",
    "gdf_mask = np.array([True if x in name_list_nlcd_all else False for x in gdf.name])\n",
    "augmented_gdf = gdf[gdf_mask]\n",
    "print(augmented_gdf.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4074b3-c210-4e61-8525-396743e7a81e",
   "metadata": {},
   "source": [
    "## for loop ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409bb40c-b7d7-4054-8a4e-b7cc00721747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2054 USGS_LPC_MN_Phase1_RedwoodCO_2010_LAS_2016. EPSG:32615: area is 2780.68 km2, density is 1.26 p/m2\n",
      "2/2054 MN_FullState. EPSG:32615: area is 244710.47 km2, density is 1.19 p/m2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(41) # set seed \n",
    "num_samples = 1000  # Number of tiles to sample \n",
    "\n",
    "# containers \n",
    "names = []\n",
    "urls = []\n",
    "local_epsg = []\n",
    "polygons = []\n",
    "lc_tile_labels = []\n",
    "slope_tile_labels = []\n",
    "skipped_index = []\n",
    "polygons_local_epsg = []\n",
    "\n",
    "# loop for each land cover (cropped by point cloud boundary)\n",
    "for i in range(len(nlcd_all)):\n",
    "    sample_nlcd = nlcd_all[i]\n",
    "    sample_dem = dem_all[name_list_dem_all.index(name_list_nlcd_all[i])]\n",
    "    \n",
    "    # get the EPSG code for this crop\n",
    "    region_data = augmented_gdf[augmented_gdf.name == name_list_nlcd_all[i]]\n",
    "    epsg_code, area_utm, (lat_center, lon_center) = reproject_to_utm_and_calculate_area(region_data)\n",
    "    density_local = augmented_gdf[augmented_gdf.name == name_list_nlcd_all[i]]['count'].values[0] / area_utm\n",
    "    dst_crs = f'EPSG:{epsg_code}'\n",
    "    print(f'{i+1}/{len(nlcd_all)} {name_list_nlcd_all[i]}. {dst_crs}: area is {area_utm/1e6:.2f} km2, density is {density_local:.2f} p/m2')\n",
    "    \n",
    "    # File Paths to DEM and land cover data\n",
    "    dem_path = sample_dem\n",
    "    land_cover_path = sample_nlcd\n",
    "    \n",
    "    # Align rasters\n",
    "    align_resolution = 25 # the resolution when they are aligned. CRS units.\n",
    "    dem, land_cover, intersection_transform = align_rasters(dem_path, land_cover_path, dst_crs, resolution=align_resolution)\n",
    "    pixel_length = np.abs(intersection_transform[0])\n",
    "    assert pixel_length == align_resolution\n",
    "    assert dem.shape == land_cover.shape\n",
    "    \n",
    "    # land cover reclassification from levle II to level I\n",
    "    reclassified_land_cover = reclassify_land_cover_urban_forest(land_cover)\n",
    "    \n",
    "    # mask out outside of AoI region of Dem with Land cover labels (artifacts due to reprojection)\n",
    "    mask = reclassified_land_cover == 0\n",
    "    dem[mask] = np.nan\n",
    "    \n",
    "    # Calculate the slope from DEM and classify into USDA classes\n",
    "    slope = calculate_slope(dem, intersection_transform)\n",
    "    classified_slope = classify_slope(slope)\n",
    "    classified_slope = reclassify_slope(classified_slope)\n",
    "    \n",
    "    # compute inverse frequnecy probability map for tile sampling \n",
    "    slope_map = classified_slope\n",
    "    land_cover_map = reclassified_land_cover\n",
    "    tile_size = int(500 // pixel_length)\n",
    "    \n",
    "    prob_map, slope_tiles, land_cover_tiles, num_patches_available, mask = process_maps_class_freq(slope_map, land_cover_map, tile_size)\n",
    "\n",
    "    if num_patches_available == 0:\n",
    "        print(f'Skip {i}th sample due to zero tiles')\n",
    "        skipped_index.append(i)\n",
    "        continue\n",
    "    \n",
    "    # Sample pixels (tiles) from the probability map\n",
    "    geotransform = intersection_transform # pixel space to geographical transform \n",
    "    sampled_indices = sample_pixels_by_probability(prob_map, num_samples)\n",
    "    if sampled_indices == 0:\n",
    "        print(f'Skip {i}th sample due to zero tiles')\n",
    "        skipped_index.append(i)\n",
    "        continue\n",
    "    \n",
    "    # Back trace the selected pixels to the original tiles and get their bounds\n",
    "    sampled_slope_tiles, sampled_bounds = back_trace_tiles(sampled_indices, slope_tiles, tile_size, geotransform)\n",
    "    sampled_land_cover_tiles, _ = back_trace_tiles(sampled_indices, land_cover_tiles, tile_size, geotransform)\n",
    "    sampled_pixel_indices = [geo_bounds_to_pixel_indices([item for subtuple in bounds for item in subtuple], geotransform) for bounds in sampled_bounds]\n",
    "    sampled_land_cover_tile_labels = get_tile_labels(sampled_land_cover_tiles)\n",
    "    sampled_slope_tile_labels = get_tile_labels(sampled_slope_tiles)\n",
    "    \n",
    "    # obtain geo bounds in EPSG 3857 for data extraction in 3DEP database \n",
    "    sampled_bounds_epsg3857 = transform_bounds_to_epsg3857(sampled_bounds, epsg_code)\n",
    "    \n",
    "    names.append(name_list_nlcd_all[i])\n",
    "    urls.append(augmented_gdf[augmented_gdf.name == name_list_nlcd_all[i]].url.values[0])\n",
    "    local_epsg.append(epsg_code)\n",
    "    polygons.append(sampled_bounds_epsg3857)\n",
    "    lc_tile_labels.append(sampled_land_cover_tile_labels)\n",
    "    slope_tile_labels.append(sampled_slope_tile_labels)\n",
    "    polygons_local_epsg.append(sampled_bounds)\n",
    "    \n",
    "    # break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d0e4a-4d42-426d-9890-dd76d81a6250",
   "metadata": {},
   "source": [
    "## parallel ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ba9936-a18f-4ef7-823a-799374ae846a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2054 USGS_LPC_MN_Phase1_RedwoodCO_2010_LAS_2016. EPSG:32615: area is 2780.68 km2, density is 1.26 p/m2\n",
      "5/2054 USGS_LPC_IL_12_County_Stephenson_Co_2009_LAS_2016. EPSG:32616: area is 1650.69 km2, density is 2.06 p/m2\n",
      "6/2054 WY_SouthCentral_5_2020. EPSG:32613: area is 4520.55 km2, density is 59.67 p/m2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hirokishuu/miniconda3/envs/geospatial/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the function to be run in parallel\n",
    "def process_sample(i):\n",
    "    sample_nlcd = nlcd_all[i]\n",
    "    sample_dem = dem_all[name_list_dem_all.index(name_list_nlcd_all[i])]\n",
    "    \n",
    "    # get the EPSG code for this region \n",
    "    region_data = augmented_gdf[augmented_gdf.name == name_list_nlcd_all[i]]\n",
    "    epsg_code, area_utm, (lat_center, lon_center) = reproject_to_utm_and_calculate_area(region_data)\n",
    "    density_local = augmented_gdf[augmented_gdf.name == name_list_nlcd_all[i]]['count'].values[0] / area_utm\n",
    "    dst_crs = f'EPSG:{epsg_code}'\n",
    "    print(f'{i+1}/{len(nlcd_all)} {name_list_nlcd_all[i]}. {dst_crs}: area is {area_utm/1e6:.2f} km2, density is {density_local:.2f} p/m2')\n",
    "    \n",
    "    # Paths to DEM and land cover data\n",
    "    dem_path = sample_dem\n",
    "    land_cover_path = sample_nlcd\n",
    "    \n",
    "    # Align rasters\n",
    "    align_resolution = 25 # the resolution when they are aligned. CRS units\n",
    "    dem, land_cover, intersection_transform = align_rasters(dem_path, land_cover_path, dst_crs, resolution=align_resolution)\n",
    "    pixel_length = np.abs(intersection_transform[0])\n",
    "    assert pixel_length == align_resolution\n",
    "    assert dem.shape == land_cover.shape\n",
    "    \n",
    "    # land cover reclassification from levle II to level I\n",
    "    reclassified_land_cover = reclassify_land_cover_urban_forest(land_cover)\n",
    "    \n",
    "    # mask out outside of AoI region of Dem with Land cover labels. (artifacts due to reprojection)\n",
    "    mask = reclassified_land_cover == 0\n",
    "    dem[mask] = np.nan\n",
    "    \n",
    "    # Calculate the slope from DEM and classify into USDA classes\n",
    "    slope = calculate_slope(dem, intersection_transform)\n",
    "    classified_slope = classify_slope(slope)\n",
    "    classified_slope = reclassify_slope(classified_slope)\n",
    "    \n",
    "    # compute inverse frequnecy probability map for tile sampling \n",
    "    slope_map = classified_slope\n",
    "    land_cover_map = reclassified_land_cover\n",
    "    tile_size = int(500 // pixel_length)  # 50x50 pixels for 500m x 500m tiles if each pixel is 10m x 10m\n",
    "    \n",
    "    prob_map, slope_tiles, land_cover_tiles, num_patches_available, mask = process_maps_class_freq(slope_map, land_cover_map, tile_size)\n",
    "\n",
    "    if num_patches_available == 0:\n",
    "        print(f'Skip {i}th sample due to zero tiles')\n",
    "        return i, None\n",
    "    # Sample pixels (tiles) from the probability map\n",
    "    geotransform = intersection_transform # pixel space to geographical transform \n",
    "    sampled_indices = sample_pixels_by_probability(prob_map, num_samples)\n",
    "    if sampled_indices == 0:\n",
    "        print(f'Skip {i}th sample due to zero tiles')\n",
    "        return i, None\n",
    "    \n",
    "    # Back trace the selected pixels to the original tiles and get their bounds\n",
    "    sampled_slope_tiles, sampled_bounds = back_trace_tiles(sampled_indices, slope_tiles, tile_size, geotransform)\n",
    "    sampled_land_cover_tiles, _ = back_trace_tiles(sampled_indices, land_cover_tiles, tile_size, geotransform)\n",
    "    sampled_pixel_indices = [geo_bounds_to_pixel_indices([item for subtuple in bounds for item in subtuple], geotransform) for bounds in sampled_bounds]\n",
    "    sampled_land_cover_tile_labels = get_tile_labels(sampled_land_cover_tiles)\n",
    "    sampled_slope_tile_labels = get_tile_labels(sampled_slope_tiles)\n",
    "    \n",
    "    # obtain geo bounds in EPSG 3857 for data extraction in 3DEP database \n",
    "    sampled_bounds_epsg3857 = transform_bounds_to_epsg3857(sampled_bounds, epsg_code)\n",
    "    \n",
    "    return i, {\n",
    "        'name': name_list_nlcd_all[i],\n",
    "        'url': augmented_gdf[augmented_gdf.name == name_list_nlcd_all[i]].url.values[0],\n",
    "        'epsg_code': epsg_code,\n",
    "        'bounds': sampled_bounds_epsg3857,\n",
    "        'lc_labels': sampled_land_cover_tile_labels,\n",
    "        'slope_labels': sampled_slope_tile_labels,\n",
    "        'local_bounds': sampled_bounds\n",
    "    }\n",
    "\n",
    "# Create lists to store results\n",
    "names = []\n",
    "urls = []\n",
    "local_epsg = []\n",
    "polygons = []\n",
    "lc_tile_labels = []\n",
    "slope_tile_labels = []\n",
    "skipped_index = []\n",
    "polygons_local_epsg = []\n",
    "\n",
    "# Set the number of CPUs\n",
    "num_cpus = 4  # Adjust this to the number of CPUs you want to use\n",
    "\n",
    "# Use joblib.Parallel to parallelize the for loop\n",
    "results = Parallel(n_jobs=num_cpus, timeout=300)(delayed(process_sample)(i) for i in range(len(nlcd_all)))\n",
    "\n",
    "# Collect results\n",
    "for i, result in results:\n",
    "    if result is None:\n",
    "        skipped_index.append(i)\n",
    "    else:\n",
    "        names.append(result['name'])\n",
    "        urls.append(result['url'])\n",
    "        local_epsg.append(result['epsg_code'])\n",
    "        polygons.append(result['bounds'])\n",
    "        lc_tile_labels.append(result['lc_labels'])\n",
    "        slope_tile_labels.append(result['slope_labels'])\n",
    "        polygons_local_epsg.append(result['local_bounds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d9b21d-3b43-4f51-831a-b440450f87d2",
   "metadata": {},
   "source": [
    "## save the sampled tiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db5b56f-3420-49e5-9e07-f2699fa37968",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "slope_tile_labels_json = [to_json_str(region_labels) for region_labels in slope_tile_labels]\n",
    "lc_tile_labels_json = [to_json_str(region_labels) for region_labels in lc_tile_labels]\n",
    "\n",
    "# Create a list of polygons from bounds\n",
    "multi_polygon_list = []\n",
    "for region_polygons in polygons: \n",
    "    # region polygons: length 100 ((minx, miny), (maxx, maxy))\n",
    "    polygon_list = [Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)]) for ((minx, miny), (maxx, maxy)) in region_polygons]\n",
    "    multi_polygon = MultiPolygon(polygon_list)\n",
    "    multi_polygon_list.append(multi_polygon)\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    'name': names,\n",
    "    'url': urls,\n",
    "    'local_epsg_code': local_epsg,\n",
    "    'slope_tile_labels': slope_tile_labels_json,\n",
    "    'land_cover_tile_labels': lc_tile_labels_json, # json string reading is much faster for gpkg compareed to int or float\n",
    "    'bounds_in_local_epsg': polygons_local_epsg,\n",
    "    'geometry': multi_polygon_list\n",
    "}\n",
    "\n",
    "# Create the GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(data, crs=\"EPSG:3857\")\n",
    "gdf.to_file(f\"../../data/sampled_tiles/sample_{num_samples}_developed_forest.gpkg\", driver=\"GPKG\", engine='pyogrio')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
